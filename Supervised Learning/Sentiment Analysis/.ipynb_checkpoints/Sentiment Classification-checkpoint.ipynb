{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_print_review_and_label(i):\n",
    "    print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")\n",
    "\n",
    "g = open('reviews.txt','r')\n",
    "reviews = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "\n",
    "g = open('labels.txt','r')\n",
    "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop a Predictive Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.txt \t : \t reviews.txt\n",
      "\n",
      "NEGATIVE\t:\tthis movie is terrible but it has some good effects .  ...\n",
      "POSITIVE\t:\tadrian pasdar is excellent is this film . he makes a fascinating woman .  ...\n",
      "NEGATIVE\t:\tcomment this movie is impossible . is terrible  very improbable  bad interpretat...\n",
      "POSITIVE\t:\texcellent episode movie ala pulp fiction .  days   suicides . it doesnt get more...\n"
     ]
    }
   ],
   "source": [
    "print(\"labels.txt \\t : \\t reviews.txt\\n\")\n",
    "pretty_print_review_and_label(2137)\n",
    "pretty_print_review_and_label(12816)\n",
    "pretty_print_review_and_label(6267)\n",
    "pretty_print_review_and_label(21934)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Theory Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_print_word_and_frequency(word):\n",
    "    positives = sum([1 if word in reviews[i] and labels[i]=='POSITIVE' else 0 for i in range(len(reviews))]) / len(reviews) * 100\n",
    "    negatives = sum([1 if word in reviews[i] and labels[i]=='NEGATIVE' else 0 for i in range(len(reviews))]) / len(reviews) * 100\n",
    "    print('%20s \\t %30s%% \\t %30s%%'  % (word, round(positives, 2), round(negatives, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Word \t                      Positives \t                      Negatives\n",
      "            terrible \t                           0.87% \t                           4.49%\n",
      "                 bad \t                           6.56% \t                          18.96%\n",
      "           excellent \t                            5.9% \t                           1.41%\n",
      "               trash \t                           0.64% \t                           1.66%\n",
      "             awesome \t                           1.26% \t                           0.48%\n"
     ]
    }
   ],
   "source": [
    "print('%20s \\t %30s \\t %30s' % ('Word', 'Positives', 'Negatives'))\n",
    "pretty_print_word_and_frequency('terrible')\n",
    "pretty_print_word_and_frequency('bad')\n",
    "pretty_print_word_and_frequency('excellent')\n",
    "pretty_print_word_and_frequency('trash')\n",
    "pretty_print_word_and_frequency('awesome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_counts = Counter()\n",
    "negative_counts = Counter()\n",
    "total_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(reviews)):    \n",
    "    for word in reviews[i].split():\n",
    "        total_counts[word] += 1\n",
    "        if labels[i] == 'POSITIVE':\n",
    "            positive_counts[word] += 1\n",
    "        else:\n",
    "            negative_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 173324),\n",
       " ('.', 159654),\n",
       " ('and', 89722),\n",
       " ('a', 83688),\n",
       " ('of', 76855),\n",
       " ('to', 66746),\n",
       " ('is', 57245),\n",
       " ('in', 50215),\n",
       " ('br', 49235),\n",
       " ('it', 48025),\n",
       " ('i', 40743),\n",
       " ('that', 35630),\n",
       " ('this', 35080),\n",
       " ('s', 33815),\n",
       " ('as', 26308),\n",
       " ('with', 23247),\n",
       " ('for', 22416),\n",
       " ('was', 21917),\n",
       " ('film', 20937),\n",
       " ('but', 20822)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"POSITIVES\")\n",
    "positive_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NEGATIVES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('.', 167538),\n",
       " ('the', 163389),\n",
       " ('a', 79321),\n",
       " ('and', 74385),\n",
       " ('of', 69009),\n",
       " ('to', 68974),\n",
       " ('br', 52637),\n",
       " ('is', 50083),\n",
       " ('it', 48327),\n",
       " ('i', 46880),\n",
       " ('in', 43753),\n",
       " ('this', 40920),\n",
       " ('that', 37615),\n",
       " ('s', 31546),\n",
       " ('was', 26291),\n",
       " ('movie', 24965),\n",
       " ('for', 21927),\n",
       " ('but', 21781),\n",
       " ('with', 20878),\n",
       " ('as', 20625)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nNEGATIVES\")\n",
    "negative_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_neg_ratios = Counter()\n",
    "for word, count in list(total_counts.most_common()):\n",
    "    if count > 100:\n",
    "        ratio = positive_counts[word] / (negative_counts[word] + 1)\n",
    "        pos_neg_ratios[word] = np.log(ratio) if ratio > 1 else -np.log(1 / (ratio + 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest POSITIVE ratio\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('edie', 4.6913478822291435),\n",
       " ('paulie', 4.0775374439057197),\n",
       " ('felix', 3.1527360223636558),\n",
       " ('polanski', 2.8233610476132043),\n",
       " ('matthau', 2.8067217286092401),\n",
       " ('victoria', 2.6810215287142909),\n",
       " ('mildred', 2.6026896854443837),\n",
       " ('gandhi', 2.5389738710582761),\n",
       " ('flawless', 2.451005098112319),\n",
       " ('superbly', 2.2600254785752498),\n",
       " ('perfection', 2.1594842493533721),\n",
       " ('astaire', 2.1400661634962708),\n",
       " ('captures', 2.0386195471595809),\n",
       " ('voight', 2.0301704926730531),\n",
       " ('wonderfully', 2.0218960560332353),\n",
       " ('powell', 1.9783454248084671),\n",
       " ('brosnan', 1.9547990964725592),\n",
       " ('lily', 1.9203768470501485),\n",
       " ('bakshi', 1.9029851043382795),\n",
       " ('lincoln', 1.9014583864844796)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Highest POSITIVE ratio\")\n",
    "pos_neg_ratios.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest NEGATIVE ratio\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('boll', -4.0778152602708904),\n",
       " ('uwe', -3.9218753018711578),\n",
       " ('seagal', -3.3202501058581921),\n",
       " ('unwatchable', -3.0269848170580955),\n",
       " ('stinker', -2.9876839403711624),\n",
       " ('mst', -2.7753833211707968),\n",
       " ('incoherent', -2.7641396677532537),\n",
       " ('unfunny', -2.5545257844967644),\n",
       " ('waste', -2.4907515123361046),\n",
       " ('blah', -2.4475792789485005),\n",
       " ('horrid', -2.3715779644809971),\n",
       " ('pointless', -2.3451073877136341),\n",
       " ('atrocious', -2.3187369339642556),\n",
       " ('redeeming', -2.2667790015910296),\n",
       " ('prom', -2.2601040980178784),\n",
       " ('drivel', -2.2476029585766928),\n",
       " ('lousy', -2.2118080125207054),\n",
       " ('worst', -2.1930856334332267),\n",
       " ('laughable', -2.172468615469592),\n",
       " ('awful', -2.1385076866397488)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Highest NEGATIVE ratio\")\n",
    "list(reversed(pos_neg_ratios.most_common()))[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Input/Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74073"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set(total_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_0 = np.zeros((1,vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_input_layer(review):\n",
    "    global layer_0\n",
    "    layer_0 *= 0\n",
    "    for word in review.split():\n",
    "        layer_0[0][word2index[word]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_target_for_label(label):\n",
    "    return int(label == \"POSITIVE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_input_to_hidden = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
    "                                       (self.hidden_nodes, self.input_nodes))\n",
    "\n",
    "        self.weights_hidden_to_output = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                       (self.output_nodes, self.hidden_nodes))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: 1.0 / (1 + np.exp(-x))\n",
    "    \n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # Convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        ### Forward pass ###\n",
    "        # Hidden layer\n",
    "        hidden_inputs = self.weights_input_to_hidden.dot(inputs) # signals into hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\n",
    "        \n",
    "        # Output layer\n",
    "        final_inputs = self.weights_hidden_to_output.dot(hidden_outputs) # signals into final output layer\n",
    "        final_outputs = self.activation_function(final_inputs) # signals from final output layer\n",
    "        \n",
    "        ### Backward pass ###\n",
    "        \n",
    "        # Output error\n",
    "        # Output layer error is the difference between desired target and actual output.\n",
    "        output_errors = (targets - final_outputs)\n",
    "        output_grad = final_outputs * (1 - final_outputs)\n",
    "        \n",
    "        # Backpropagated error\n",
    "        # errors propagated to the hidden layer\n",
    "        hidden_errors = self.weights_hidden_to_output.T.dot(output_errors)\n",
    "        hidden_grad = hidden_outputs * (1 - hidden_outputs) # hidden layer gradients\n",
    "    \n",
    "        # Update the weights\n",
    "        # update hidden-to-output weights with gradient descent step\n",
    "        self.weights_hidden_to_output += np.outer(output_errors * output_grad, hidden_outputs) * self.lr\n",
    "        # update input-to-hidden weights with gradient descent step\n",
    "        self.weights_input_to_hidden += np.outer(hidden_errors * hidden_grad, inputs) * self.lr\n",
    " \n",
    "        \n",
    "    def run(self, inputs_list):\n",
    "        # Run a forward pass through the network\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # Hidden layer\n",
    "        hidden_inputs = self.weights_input_to_hidden.dot(inputs) # signals into hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\n",
    "        \n",
    "        # Output layer\n",
    "        final_inputs = self.weights_hidden_to_output.dot(hidden_outputs) # signals into final output layer\n",
    "        final_outputs = self.activation_function(final_inputs) # signals from final output layer\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0% ... Training error: 0.498 ... Validation error: 0.499"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-0391b3987c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\rProgress: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m                      \u001b[1;33m+\u001b[0m \u001b[1;34m\"% ... Training error: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m                      \u001b[1;33m+\u001b[0m \u001b[1;34m\" ... Validation error: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'validation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loss' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# input: reviews, labels, update_input_layer(review), get_target_for_label(label), layer_0\n",
    "\n",
    "### Set the hyperparameters here ###\n",
    "epochs = 10\n",
    "learning_rate = 0.1 \n",
    "hidden_nodes = 10 \n",
    "output_nodes = 1 \n",
    "\n",
    "N_i = vocab_size\n",
    "network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "train_reviews, test_reviews, train_labels, test_labels = train_test_split(\n",
    "    reviews, labels, train_size=0.7, random_state=42)\n",
    "\n",
    "losses = {'train':[], 'validation':[]} # split\n",
    "for e in range(epochs): \n",
    "    # Go through a random batch of 128 records from the training data set\n",
    "    batch = np.random.choice(range(len(train_reviews)), size=512)\n",
    "    for i in batch:\n",
    "        sys.stdout.write(\"\\rBatch no: \" + str(i))\n",
    "        update_input_layer(train_reviews[i])\n",
    "        network.train(layer_0, get_target_for_label(train_labels[i]))\n",
    "    \n",
    "    y_pred = []\n",
    "    y = []\n",
    "    for i in range(len(train_reviews)):\n",
    "        sys.stdout.write(\"\\rTrain review no: \" + str(i))\n",
    "        update_input_layer(train_reviews[i])\n",
    "        y_pred += [int(np.asscalar(network.run(layer_0)) > 0.5)]\n",
    "        y += [get_target_for_label(train_labels[i])]\n",
    "    train_error = 1 - accuracy_score(y_pred, y)\n",
    "    \n",
    "    y_pred = []\n",
    "    y = []\n",
    "    for i in range(len(test_reviews)):\n",
    "        sys.stdout.write(\"\\rTest review no: \" + str(i))\n",
    "        update_input_layer(test_reviews[i])\n",
    "        y_pred += [int(np.asscalar(network.run(layer_0)) > 0.5)]\n",
    "        y += [get_target_for_label(test_labels[i])]\n",
    "    val_error = 1 - accuracy_score(y_pred, y)\n",
    "    \n",
    "    # Printing out the training progress\n",
    "    sys.stdout.write(\"\\rProgress: \" + str(100 * e/float(epochs))[:4] \\\n",
    "                     + \"% ... Training error: \" + str(train_error)[:5] \\\n",
    "                     + \" ... Validation error: \" + str(val_error)[:5])\n",
    "    \n",
    "    losses['train'].append(train_error)\n",
    "    losses['validation'].append(val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
